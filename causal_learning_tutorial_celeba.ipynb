{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Causal Learning Tutorial - CelebA***\n",
    "\n",
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Basic Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "def set_random_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    return\n",
    "\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Load Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1. CelebA Dataset\n",
    "\n",
    "The CelebA (CelebFaces Attributes Dataset) is a large-scale face attributes dataset widely used in facial recognition research. It includes 202,599 face images of 10,177 celebrities, each annotated with 40 attribute labels and 5 landmark locations. This dataset is extensively utilized in various computer vision research fields, particularly for facial recognition tasks.\n",
    "\n",
    "##### Dataset Features:\n",
    "- **Number of Images**: 202,599\n",
    "- **Number of Celebrities**: 10,177\n",
    "- **Number of Attributes**: 40 facial attributes\n",
    "- **Landmarks**: 5 landmark locations per image\n",
    "\n",
    "CelebA is particularly useful for studies in facial recognition, facial attribute detection, landmark localization, and attribute-based image retrieval. The diversity and size of the dataset make it suitable for training complex facial recognition models.\n",
    "\n",
    "##### Licensing and More Information:\n",
    "The CelebA dataset is available under a license that permits its use for academic and non-commercial purposes. For detailed documentation and to access the dataset, please visit the [CelebA Dataset Docs](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_img_sz = 128 # Image size\n",
    "attribute_indices = torch.tensor([20, 31]) # Male, Smiling\n",
    "\n",
    "trainset = dset.CelebA(root='../data/celeba', split = \"train\", transform=transforms.Compose([\n",
    "                            transforms.Resize(n_img_sz), # Transformations include resizing the images to `n_img_sz`\n",
    "                            transforms.CenterCrop(n_img_sz), # Center cropping to the same size\n",
    "                            transforms.ToTensor(), # Converting the images to tensors,\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Normalizing the pixel values to have a mean and standard deviation of 0.5 across all channels.\n",
    "                        ]), download= False)\n",
    "testset = dset.CelebA(root='../data/celeba', split = \"test\", transform=transforms.Compose([\n",
    "                            transforms.Resize(n_img_sz), # Transformations include resizing the images to `n_img_sz`\n",
    "                            transforms.CenterCrop(n_img_sz), # Center cropping to the same size\n",
    "                            transforms.ToTensor(), # Converting the images to tensors\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Normalizing the pixel values to have a mean and standard deviation of 0.5 across all channels.\n",
    "                        ]), download= False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. Dataloader Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Dataloader(trainset, batch_size, shuffle = False, num_workers = 0, collate = None):\n",
    "    return torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle = shuffle, num_workers = num_workers, collate_fn=collate, drop_last=True)\n",
    "\n",
    "def get_testloader(testset, batch_size, shuffle = False, num_workers = 0, collate = None):\n",
    "    return torch.utils.data.DataLoader(testset, batch_size= batch_size, shuffle = shuffle, num_workers = num_workers, collate_fn=collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-3. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.dataset[index] # Get the image and label at the specified index\n",
    "        y= torch.index_select(y.unsqueeze(0), 1, attribute_indices).squeeze(0) # Select specific attributes(Male, Smiling) for the label using a predefined list of indices\n",
    "        return X, y # Return the image and the selected attributes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) # Return the size of the dataset\n",
    "    \n",
    "trainset = CelebA(trainset)\n",
    "testset = CelebA(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-4. Training Setup and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape=[3, n_img_sz, n_img_sz]\n",
    "label_size = 2\n",
    "explain_size = 256\n",
    "batch_size = 32\n",
    "selected_indices = [737, 1518, 390, 607] # selected people's images\n",
    "\n",
    "num_epoch = 5\n",
    "learning_rate = 2e-4\n",
    "total_iters = (len(trainset)//batch_size) * num_epoch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Custom Style Gan Classes**\n",
    "\n",
    "##### This notebook demonstrates how to customize StyleGAN components, utilizing the discriminator as both the explainer and reasoner, and\n",
    "##### the generator as the producer in a causal cooperative network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_size, num_channels = 3, channel_multiplier=64):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, channel_multiplier, 3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(channel_multiplier, channel_multiplier * 2, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(channel_multiplier * 2, channel_multiplier * 4, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(channel_multiplier * 4, channel_multiplier * 8, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channel_multiplier * 8 * 1 * 1, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.main(img)\n",
    "\n",
    "class ConditionStyleMod(nn.Module):\n",
    "    def __init__(self, latent_dim, condition_dim, channels):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(latent_dim + condition_dim, channels * 2)\n",
    "\n",
    "    def forward(self, x, latent, condition):\n",
    "        style = self.lin(torch.cat([latent, condition], dim=1))\n",
    "        scale = style[:, :x.size(1)].unsqueeze(2).unsqueeze(3)\n",
    "        shift = style[:, x.size(1):].unsqueeze(2).unsqueeze(3)\n",
    "        return x * (scale + 1) + shift\n",
    "\n",
    "class ConditionConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, latent_dim, condtion_dim, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.style1 = ConditionStyleMod(latent_dim, condtion_dim, out_channels)\n",
    "        self.style2 = ConditionStyleMod(latent_dim, condtion_dim, out_channels)\n",
    "        self.noise1 = nn.Parameter(torch.zeros(1, out_channels, 1, 1))\n",
    "        self.noise2 = nn.Parameter(torch.zeros(1, out_channels, 1, 1))\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, latent, condition):\n",
    "        x = self.conv1(x)\n",
    "        rand1 = torch.randn_like(x)\n",
    "        x = x + self.noise1 * rand1\n",
    "        x = self.act(x)\n",
    "        x = self.style1(x, latent, condition)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        rand2 = torch.randn_like(x)\n",
    "        x = x + self.noise2 * rand2\n",
    "        x = self.act(x)\n",
    "        x = self.style2(x, latent, condition)\n",
    "        return x\n",
    "\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, condition_dim, z_dim, channel_multiplier=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial = nn.Parameter(torch.randn(1, channel_multiplier, 4, 4))\n",
    "        self.style1 = ConditionStyleMod(z_dim, condition_dim, channel_multiplier)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ConditionConvBlock(channel_multiplier, channel_multiplier, z_dim, condition_dim, 3, 1),\n",
    "            ConditionConvBlock(channel_multiplier, channel_multiplier, z_dim, condition_dim, 3, 1),\n",
    "            ConditionConvBlock(channel_multiplier, channel_multiplier, z_dim, condition_dim, 3, 1),\n",
    "            ConditionConvBlock(channel_multiplier, channel_multiplier, z_dim, condition_dim, 3, 1),\n",
    "            ConditionConvBlock(channel_multiplier, channel_multiplier, z_dim, condition_dim, 3, 1),\n",
    "        ])\n",
    "        self.to_rgb = nn.Sequential(\n",
    "            nn.Conv2d(channel_multiplier, 3, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, condition, style):\n",
    "        batch_size = style.shape[0]\n",
    "        out = self.initial.expand(batch_size, -1, -1, -1)\n",
    "        out = self.style1(out, style, condition)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "            out = block(out, style, condition)\n",
    "\n",
    "        out = self.to_rgb(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Class Image Debugger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDebugger:\n",
    "    def __init__(self, dataset, label_size, device, selected_indices=None):\n",
    "        self.device = device\n",
    "        self.label_size = label_size\n",
    "        self.selected_indices = selected_indices or []\n",
    "        self.n_canvas_col = len(self.selected_indices)\n",
    "        self.n_canvas_row = min(self.n_canvas_col, 4)\n",
    "        self.display_images, self.display_labels = self.stack_selected_images_labels(dataset)\n",
    "        self.canvas_image = self.create_canvas()\n",
    "        \n",
    "    def stack_selected_images_labels(self, dataset):\n",
    "        images, labels = [], []\n",
    "        for idx in self.selected_indices:\n",
    "            img, lbl = dataset[idx]\n",
    "            images.append(img.unsqueeze(0))\n",
    "            labels.append(lbl.unsqueeze(0).type(torch.float))\n",
    "        \n",
    "        return torch.cat(images).to(self.device).float(), torch.cat(labels).to(self.device).float()\n",
    "\n",
    "    def create_canvas(self):\n",
    "        canvas = np.ones((n_img_sz * (self.n_canvas_row + 1), \n",
    "                          n_img_sz * (self.n_canvas_col + 1), 3))\n",
    "        for i, image in enumerate(self.display_images):\n",
    "            img = np.transpose(vutils.make_grid(image.detach().cpu().unsqueeze(0), padding=0, normalize=True).numpy(), (1, 2, 0))\n",
    "            canvas[:n_img_sz, n_img_sz * (i + 1):n_img_sz * (i + 2)] = img\n",
    "        return canvas\n",
    "\n",
    "    def update_images(self):\n",
    "        with torch.no_grad():\n",
    "            explains = explainer(self.display_images)\n",
    "            inferred_labels = reasoner(self.display_images, explains)\n",
    "            for i in range(self.n_canvas_row):\n",
    "                controlled_labels = self.display_labels[i:i + 1, :].clone().detach().expand_as(inferred_labels)\n",
    "                generated_images = producer(controlled_labels, explains).cpu()\n",
    "                self.place_on_canvas(generated_images, i)\n",
    "\n",
    "    def place_on_canvas(self, images, row):\n",
    "        img = np.transpose(vutils.make_grid(images[:self.n_canvas_col], padding=0, normalize=True).numpy(), (1, 2, 0))\n",
    "        self.canvas_image[n_img_sz * (row + 1):n_img_sz * (row + 2), n_img_sz:] = img\n",
    "\n",
    "    def display_image(self, figsize=(13, 13)):\n",
    "        plt.figure(figsize=figsize)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.imshow(self.canvas_image)\n",
    "        plt.axis(\"off\")\n",
    "        labels = [\"Female, No-smile\", \"Male, No-smile\", \"Female, Smile\", \"Male, Smile\"]\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.text(60, 128 * (i + 2) - 128 // 2, label, fontsize=12, va='center', ha='center')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Causal Cooperative Network(CCNet)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-1. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(param_object):\n",
    "    if isinstance(param_object, Iterable):\n",
    "        for layer in param_object:\n",
    "            if isinstance(layer, nn.Linear) or isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-2. Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explainer(nn.Module):\n",
    "    def __init__(self, obs_shape, explain_size):\n",
    "        super(Explainer, self).__init__()\n",
    "        self.net = Discriminator(explain_size, num_channels=3) \n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.net(x) \n",
    "        e = torch.sigmoid(e)\n",
    "        return e\n",
    "    \n",
    "class Reasoner(nn.Module):\n",
    "    def __init__(self, obs_shape, explain_size, label_size):\n",
    "        super(Reasoner, self).__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.explain_size = explain_size\n",
    "        self.total_product = torch.prod(torch.tensor(obs_shape[1:], dtype=torch.int)).item()\n",
    "        self.net = Discriminator(label_size, num_channels=4) \n",
    "        self.apply(init_weights)\n",
    "    \n",
    "    def forward(self, obs, e):\n",
    "        e = self._convert_explanation_to_input_shape(e)\n",
    "        z = torch.cat([obs, e], dim=1)  # Concatenate along the channel dimension\n",
    "        y = self.net(z) \n",
    "        y = torch.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "    def _convert_explanation_to_input_shape(self, e):\n",
    "        \"\"\" Convert the explanation vector to match the target image shape with the first dimension set to 1. \"\"\"\n",
    "        explain_shape = [1] + list(self.obs_shape[1:])  # Set first dim to 1, rest match target shape\n",
    "        expanded_e = torch.cat([e.repeat(1, self.total_product//self.explain_size), torch.zeros_like(e[:, :self.total_product%self.explain_size])], dim = -1)  # Repeat to match the volume of target shape\n",
    "        expanded_e = expanded_e.view(-1, *explain_shape)  # Reshape explanation vector to the new explain_shape\n",
    "        return expanded_e\n",
    "\n",
    "class Producer(nn.Module):\n",
    "    def __init__(self, label_size, explain_size, obs_shape):\n",
    "        super(Producer, self).__init__()\n",
    "        self.net = ConditionalGenerator(condition_dim=label_size, z_dim=explain_size)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, labels, explains):\n",
    "        x = self.net(condition = labels, style = explains) \n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = Explainer(obs_shape, explain_size).to(device)\n",
    "reasoner = Reasoner(obs_shape, explain_size, label_size).to(device)\n",
    "producer = Producer(label_size, explain_size, obs_shape).to(device)\n",
    "\n",
    "networks = [explainer, reasoner, producer] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. CCNet Training Parameters and Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "decay_rate = 0.1\n",
    "iteration_100k = 100000\n",
    "gamma = pow(decay_rate, 1 / iteration_100k)\n",
    "\n",
    "# Components setup\n",
    "components = {\n",
    "    'explainer': explainer.parameters(),\n",
    "    'reasoner': reasoner.parameters(),\n",
    "    'producer': producer.parameters()\n",
    "}\n",
    "\n",
    "# Optimizers and Schedulers\n",
    "optimizers = {}\n",
    "schedulers = {}\n",
    "\n",
    "for name, parameters in components.items():\n",
    "    optimizer = optim.Adam(parameters, lr=learning_rate, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    optimizers[name] = optimizer\n",
    "    schedulers[name] = scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step():\n",
    "    # Update all optimizers\n",
    "    for optimizer in optimizers.values():\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Update all schedulers\n",
    "    for scheduler in schedulers.values():\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. CCNet Train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Training Print Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_iter(epoch, num_epoch, iters, len_dataloader, et):\n",
    "    print('[%d/%d][%d/%d][Time %.2f]'\n",
    "        % (epoch, num_epoch, iters, len_dataloader, et))\n",
    "\n",
    "def print_train(losses, errors):\n",
    "    print('--------------------Training Metrics--------------------')\n",
    "    print('Inf: %.4f\\tGen: %.4f\\tRec: %.4f\\tE: %.4f\\tR: %.4f\\tP: %.4f'\n",
    "        % (losses['inference_loss'].mean(), losses['generation_loss'].mean(), losses['reconstruction_loss'].mean(), \n",
    "           errors['explainer_error'].mean(), errors['reasoner_error'].mean(), errors['producer_error'].mean()))\n",
    "\n",
    "def print_lr():\n",
    "    opt_explainer = optimizers['explainer']\n",
    "    lr = opt_explainer.param_groups[0]['lr']\n",
    "    print('Opt-{0} lr_ERP: {1}'.format(type(opt_explainer).__name__, lr))\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print('--------------------Test Metrics------------------------')\n",
    "    for label_type, metric in metrics.items():\n",
    "        text = (f'{label_type}: %.4f'\n",
    "            % metric)\n",
    "        print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards(networks, network_errors):\n",
    "    # Implementation details follow\n",
    "    num_network = len(networks)\n",
    "    # Temporarily disable gradient computation for all networks\n",
    "    for network in networks:\n",
    "        network.requires_grad_(False)        \n",
    "        \n",
    "    for net_idx, (network, error) in enumerate(zip(networks, network_errors)):\n",
    "        # Enable gradient computation for the current network\n",
    "        network.requires_grad_(True)\n",
    "        # Zero out the gradient for all networks starting from the current network\n",
    "        for n in networks[net_idx:]:\n",
    "            n.zero_grad()\n",
    "        # Decide whether to retain the computation graph based on the network and error index\n",
    "        retain_graph = (net_idx < num_network - 1)\n",
    "        # Apply the discounted gradients in the backward pass\n",
    "        error.backward(retain_graph=retain_graph)\n",
    "        # Prevent gradient updates for the current network after its errors have been processed\n",
    "        network.requires_grad_(False)\n",
    "        \n",
    "    # Restore gradient computation capability for all networks for potential future forward passes\n",
    "    for network in networks:\n",
    "        network.requires_grad_(True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8-3. Loss and Error Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(predict, target):\n",
    "    batch_size = predict.size(0)\n",
    "    discrepancy = (predict - target.detach()).abs()\n",
    "    prediction_loss = discrepancy.view(batch_size, -1).mean(dim=-1)\n",
    "    return prediction_loss\n",
    "\n",
    "def error_fn(predict, target):\n",
    "    discrepancy = (predict - target.detach()).abs()\n",
    "    cooperative_error = discrepancy.mean(dim = 0) \n",
    "    return cooperative_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8-4. CCNet Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(networks, observation, label, iters):\n",
    "    # Set the models to training mode and perform the forward pass.\n",
    "    for network in networks:\n",
    "        network.train()\n",
    "    ################################  Forward Pass  ################################################\n",
    "    explain = explainer(observation)\n",
    "    inferred_label = reasoner(observation, explain)\n",
    "    \n",
    "    # reset random seed for internal noise factor\n",
    "    set_random_seed(iters)\n",
    "    generated_observation = producer(label, explain)\n",
    "    \n",
    "    # reset random seed for internal noise factor\n",
    "    set_random_seed(iters)\n",
    "    reconstructed_observation = producer(inferred_label, explain.detach())\n",
    "\n",
    "    ################################  Prediction Losses  ###########################################\n",
    "    # Calculate prediction losses for inference, generation, and reconstruction.\n",
    "    inference_loss = loss_fn(reconstructed_observation, generated_observation)\n",
    "    generation_loss = loss_fn(generated_observation , observation)\n",
    "    reconstruction_loss = loss_fn(reconstructed_observation, observation)\n",
    "\n",
    "    ################################  Model Losses  ################################################\n",
    "    # Calculate model errors based on the losses.\n",
    "    explainer_error = error_fn(inference_loss + generation_loss, reconstruction_loss)\n",
    "    reasoner_error = error_fn(reconstruction_loss + inference_loss, generation_loss)\n",
    "    producer_error = error_fn(generation_loss + reconstruction_loss, inference_loss)\n",
    "\n",
    "    ################################  Backward Pass  ###############################################\n",
    "    # Perform the backward pass and update the models.\n",
    "    backwards(\n",
    "        [explainer, reasoner, producer], \n",
    "        [explainer_error, reasoner_error, producer_error]\n",
    "        )\n",
    "    ################################  Model Update  ###############################################\n",
    "    # Update optimizers and schedulers, and reset the random seed.\n",
    "\n",
    "    update_step()\n",
    "\n",
    "    # Calculate and return the mean losses and errors.\n",
    "    metrics = {\n",
    "        \"losses\": \n",
    "            {\n",
    "            \"inference_loss\" : inference_loss,\n",
    "            \"generation_loss\" : generation_loss,\n",
    "            \"reconstruction_loss\" : reconstruction_loss},\n",
    "        \"errors\": \n",
    "            {\n",
    "            \"explainer_error\" : explainer_error, \n",
    "            \"reasoner_error\" : reasoner_error, \n",
    "            \"producer_error\" : producer_error}\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. CCNet Test Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_evaluation_mode():\n",
    "    for network in networks:\n",
    "        network.eval()\n",
    "\n",
    "def select_random_batch():\n",
    "    dataloader = get_testloader(testset, batch_size)\n",
    "    dataloader_list = list(dataloader)\n",
    "    random_batch = random.choice(dataloader_list)\n",
    "    source_batch, target_batch = map(lambda x: x.float().to(device), random_batch)\n",
    "    return source_batch, target_batch\n",
    "\n",
    "def compute_metrics(predicted, actual):\n",
    "    metrics = {}\n",
    "    classes = torch.unique(actual)\n",
    "    precision_list, recall_list, f1_list = [], [], []\n",
    "\n",
    "    for c in classes:\n",
    "        TP = torch.sum((predicted == c) & (actual == c)).float()\n",
    "        FP = torch.sum((predicted == c) & (actual != c)).float()\n",
    "        FN = torch.sum((predicted != c) & (actual == c)).float()\n",
    "\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1_score)\n",
    "\n",
    "    metrics['accuracy'] = torch.mean((predicted == actual).float()).item()\n",
    "    metrics['precision'] = torch.mean(torch.tensor(precision_list)).item()\n",
    "    metrics['recall'] = torch.mean(torch.tensor(recall_list)).item()\n",
    "    metrics['f1_score'] = torch.mean(torch.tensor(f1_list)).item()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def test(iters):\n",
    "    set_random_seed(iters)\n",
    "    set_evaluation_mode()\n",
    "    source_batch, target_batch = select_random_batch()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        e = explainer(source_batch)\n",
    "        inferred_y = reasoner(source_batch, e)\n",
    "    \n",
    "    inferred_labels = torch.argmax(inferred_y, dim=1)\n",
    "    actual_labels = torch.argmax(target_batch, dim=1)\n",
    "\n",
    "    metrics = compute_metrics(inferred_labels, actual_labels)\n",
    "    print_metrics(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10. CCNet Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_losses, sum_errors = None, None\n",
    "iters, cnt_checkpoints, cnt_print = 0, 0, 0\n",
    "pvt_time = time.time()\n",
    "num_checkpoints = 50\n",
    "current_lrs = []\n",
    "selected_indices = [737, 1518, 390, 607]\n",
    "image_debugger = ImageDebugger(trainset, label_size, device, selected_indices=selected_indices)\n",
    "\n",
    "pbar_epoch = tqdm(range(num_epoch), desc='Epochs', leave=False)\n",
    "for epoch in pbar_epoch:\n",
    "    dataloader = get_Dataloader(trainset, batch_size, shuffle=True)\n",
    "    len_dataloader = len(dataloader)\n",
    "    \n",
    "    # Training loop over each batch\n",
    "    pbar_iter = tqdm(dataloader, desc='Iterations', leave=False)\n",
    "    for iters, (source_batch, target_batch) in enumerate(pbar_iter):\n",
    "\n",
    "        set_random_seed(iters)\n",
    "        state_trajectory, target_trajectory = source_batch.float().to(device), target_batch.float().to(device)\n",
    "        \n",
    "        causal_trainer_metrics = train_models(networks, state_trajectory, target_trajectory, iters)\n",
    "        \n",
    "        if cnt_checkpoints % num_checkpoints == 0 and cnt_checkpoints != 0:\n",
    "            # =====process_checkpoint======\n",
    "            image_debugger.update_images()\n",
    "            image_debugger.display_image()\n",
    "\n",
    "            cur_time = time.time()\n",
    "            et = cur_time - pvt_time\n",
    "            pvt_time = cur_time\n",
    "            \n",
    "            print_iter(epoch, num_epoch, iters, len_dataloader, et)\n",
    "            print_lr()\n",
    "            print_train(causal_trainer_metrics[\"losses\"], causal_trainer_metrics[\"errors\"])\n",
    "\n",
    "            metrics = test(iters) \n",
    "\n",
    "            sum_losses, sum_errors, cnt_checkpoints = None, None, 0\n",
    "            cnt_print += 1\n",
    "            \n",
    "            pbar_iter.display()\n",
    "            pbar_epoch.display()\n",
    "\n",
    "        iters += 1; cnt_checkpoints += 1\n",
    "        total_iter = (epoch + 1) * iters\n",
    "        \n",
    "        time_cost = time.time() - pvt_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
