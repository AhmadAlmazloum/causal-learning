{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ccn-team\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "path_append = \"../\"\n",
        "sys.path.append(path_append)  # Go up one directory from where you are.\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import glob\n",
        "import tqdm\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n    TRD_DD : Date\\n    ISU_CD : Stock Code\\n    ISU_NM : Stock Name\\n    TDD_CLSPRC : Closing Price\\n    TDD_OPNPRC : Opening Price\\n    TDD_HGPRC : High Price\\n    TDD_LWPRC : Low Price\\n    MKTCAP : Market Capitalization\\n    ACC_TRDVOL : Trading Volume\\n    EPS : Earnings Per Share\\n    PER : Price-Earnings Ratio\\n    BPS : Book Value Per Share\\n    PBR : Price-Book Ratio\\n    DPS : Dividends Per Share\\n    DVD_YLD : Dividend Yield\\n\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "    TRD_DD : Date\n",
        "    ISU_CD : Stock Code\n",
        "    ISU_NM : Stock Name\n",
        "    TDD_CLSPRC : Closing Price\n",
        "    TDD_OPNPRC : Opening Price\n",
        "    TDD_HGPRC : High Price\n",
        "    TDD_LWPRC : Low Price\n",
        "    MKTCAP : Market Capitalization\n",
        "    ACC_TRDVOL : Trading Volume\n",
        "    EPS : Earnings Per Share\n",
        "    PER : Price-Earnings Ratio\n",
        "    BPS : Book Value Per Share\n",
        "    PBR : Price-Book Ratio\n",
        "    DPS : Dividends Per Share\n",
        "    DVD_YLD : Dividend Yield\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TRD_DD</th>\n",
              "      <th>ISU_CD</th>\n",
              "      <th>ISU_NM</th>\n",
              "      <th>TDD_CLSPRC</th>\n",
              "      <th>TDD_OPNPRC</th>\n",
              "      <th>TDD_HGPRC</th>\n",
              "      <th>TDD_LWPRC</th>\n",
              "      <th>MKTCAP</th>\n",
              "      <th>ACC_TRDVOL</th>\n",
              "      <th>EPS</th>\n",
              "      <th>...</th>\n",
              "      <th>BPS</th>\n",
              "      <th>PBR</th>\n",
              "      <th>DPS</th>\n",
              "      <th>DVD_YLD</th>\n",
              "      <th>GDC_sig</th>\n",
              "      <th>RSI_sig</th>\n",
              "      <th>ROC_sig</th>\n",
              "      <th>MAP_sig</th>\n",
              "      <th>STC_sig</th>\n",
              "      <th>TREND</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022/01/11</td>\n",
              "      <td>900110</td>\n",
              "      <td>이스트아시아홀딩스</td>\n",
              "      <td>145</td>\n",
              "      <td>150</td>\n",
              "      <td>151</td>\n",
              "      <td>143</td>\n",
              "      <td>24,719,670,905</td>\n",
              "      <td>4,147,345</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022/01/10</td>\n",
              "      <td>900110</td>\n",
              "      <td>이스트아시아홀딩스</td>\n",
              "      <td>150</td>\n",
              "      <td>152</td>\n",
              "      <td>155</td>\n",
              "      <td>149</td>\n",
              "      <td>25,572,073,350</td>\n",
              "      <td>2,628,028</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022/01/07</td>\n",
              "      <td>900110</td>\n",
              "      <td>이스트아시아홀딩스</td>\n",
              "      <td>151</td>\n",
              "      <td>158</td>\n",
              "      <td>162</td>\n",
              "      <td>148</td>\n",
              "      <td>25,742,553,839</td>\n",
              "      <td>7,561,654</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022/01/06</td>\n",
              "      <td>900110</td>\n",
              "      <td>이스트아시아홀딩스</td>\n",
              "      <td>158</td>\n",
              "      <td>172</td>\n",
              "      <td>186</td>\n",
              "      <td>156</td>\n",
              "      <td>26,935,917,262</td>\n",
              "      <td>22,931,278</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022/01/05</td>\n",
              "      <td>900110</td>\n",
              "      <td>이스트아시아홀딩스</td>\n",
              "      <td>171</td>\n",
              "      <td>164</td>\n",
              "      <td>187</td>\n",
              "      <td>161</td>\n",
              "      <td>29,152,163,619</td>\n",
              "      <td>29,419,967</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       TRD_DD  ISU_CD     ISU_NM TDD_CLSPRC TDD_OPNPRC TDD_HGPRC TDD_LWPRC  \\\n",
              "0  2022/01/11  900110  이스트아시아홀딩스        145        150       151       143   \n",
              "1  2022/01/10  900110  이스트아시아홀딩스        150        152       155       149   \n",
              "2  2022/01/07  900110  이스트아시아홀딩스        151        158       162       148   \n",
              "3  2022/01/06  900110  이스트아시아홀딩스        158        172       186       156   \n",
              "4  2022/01/05  900110  이스트아시아홀딩스        171        164       187       161   \n",
              "\n",
              "           MKTCAP  ACC_TRDVOL  EPS  ...  BPS  PBR  DPS DVD_YLD GDC_sig  \\\n",
              "0  24,719,670,905   4,147,345  NaN  ...  NaN  NaN  NaN     NaN       0   \n",
              "1  25,572,073,350   2,628,028  NaN  ...  NaN  NaN  NaN     NaN       0   \n",
              "2  25,742,553,839   7,561,654  NaN  ...  NaN  NaN  NaN     NaN       0   \n",
              "3  26,935,917,262  22,931,278  NaN  ...  NaN  NaN  NaN     NaN       0   \n",
              "4  29,152,163,619  29,419,967  NaN  ...  NaN  NaN  NaN     NaN       0   \n",
              "\n",
              "   RSI_sig  ROC_sig  MAP_sig  STC_sig  TREND  \n",
              "0        0        0        0        0    NaN  \n",
              "1        0        0        0        0    NaN  \n",
              "2        0        0        0        0    NaN  \n",
              "3        0        0        0        0    NaN  \n",
              "4        0       -1        0        0    NaN  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def load_and_merge_csv_files(data_directory, preprocessed_directory, file_limit=None):\n",
        "    data_files = glob.glob(os.path.join(data_directory, \"*.csv\"))\n",
        "    preprocessed_files = glob.glob(os.path.join(preprocessed_directory, \"*.csv\"))\n",
        "    \n",
        "    data_files = data_files[:file_limit]\n",
        "    preprocessed_files = preprocessed_files[:file_limit]\n",
        "    \n",
        "    merged_dfs = []\n",
        "    \n",
        "    for data_file in data_files:\n",
        "        file_name = os.path.basename(data_file)\n",
        "        \n",
        "        preprocessed_file_name = file_name.replace('.csv', '_preprocessed.csv')\n",
        "        preprocessed_file_path = os.path.join(preprocessed_directory, preprocessed_file_name)\n",
        "        \n",
        "        if preprocessed_file_path in preprocessed_files:\n",
        "            df_data = pd.read_csv(data_file)\n",
        "            df_preprocessed = pd.read_csv(preprocessed_file_path)\n",
        "            \n",
        "            merged_df = pd.merge(df_data, df_preprocessed, on='TRD_DD', suffixes=('_data', '_preprocessed'))\n",
        "            merged_dfs.append(merged_df)\n",
        "    \n",
        "    total_df = pd.concat(merged_dfs, ignore_index=True)\n",
        "    \n",
        "    return total_df\n",
        "\n",
        "data_directory = path_append + \"../data/KR_Data/data\"\n",
        "preprocessed_directory = path_append + \"../data/KR_Data/preprocessed\"\n",
        "total_df = load_and_merge_csv_files(data_directory, preprocessed_directory)\n",
        "\n",
        "\n",
        "total_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming total_df is already defined and merged from previous steps\n",
        "\n",
        "# Reverse the DataFrame to sort dates from past to present\n",
        "total_df = total_df[::-1].reset_index(drop=True)\n",
        "\n",
        "# Split the \"TRD_DD\" column into year, month, and day columns\n",
        "total_df[[\"Y\", \"M\", \"D\"]] = total_df[\"TRD_DD\"].str.split(\"/\", expand=True)\n",
        "\n",
        "# Drop the original \"TRD_DD\" column\n",
        "total_df = total_df.drop(\"TRD_DD\", axis=1)\n",
        "\n",
        "# Rearrange columns to have year, month, and day first\n",
        "total_df = total_df[[\"Y\", \"M\", \"D\"] + total_df.columns[:-3].to_list()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count_day</th>\n",
              "      <th>ISU_CD</th>\n",
              "      <th>ISU_NM</th>\n",
              "      <th>TDD_CLSPRC</th>\n",
              "      <th>TDD_OPNPRC</th>\n",
              "      <th>TDD_HGPRC</th>\n",
              "      <th>TDD_LWPRC</th>\n",
              "      <th>MKTCAP</th>\n",
              "      <th>ACC_TRDVOL</th>\n",
              "      <th>EPS</th>\n",
              "      <th>...</th>\n",
              "      <th>BPS</th>\n",
              "      <th>PBR</th>\n",
              "      <th>DPS</th>\n",
              "      <th>DVD_YLD</th>\n",
              "      <th>GDC_sig</th>\n",
              "      <th>RSI_sig</th>\n",
              "      <th>ROC_sig</th>\n",
              "      <th>MAP_sig</th>\n",
              "      <th>STC_sig</th>\n",
              "      <th>TREND</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-04-21</th>\n",
              "      <td>5468</td>\n",
              "      <td>900100</td>\n",
              "      <td>뉴프라이드(Reg.S)</td>\n",
              "      <td>12,750</td>\n",
              "      <td>15,000</td>\n",
              "      <td>17,250</td>\n",
              "      <td>12,750</td>\n",
              "      <td>89,250,000,000</td>\n",
              "      <td>2,186,517</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-04-22</th>\n",
              "      <td>5469</td>\n",
              "      <td>900100</td>\n",
              "      <td>뉴프라이드(Reg.S)</td>\n",
              "      <td>10,850</td>\n",
              "      <td>10,850</td>\n",
              "      <td>10,850</td>\n",
              "      <td>10,850</td>\n",
              "      <td>75,950,000,000</td>\n",
              "      <td>47,396</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-04-23</th>\n",
              "      <td>5470</td>\n",
              "      <td>900100</td>\n",
              "      <td>뉴프라이드(Reg.S)</td>\n",
              "      <td>9,280</td>\n",
              "      <td>9,470</td>\n",
              "      <td>10,800</td>\n",
              "      <td>9,250</td>\n",
              "      <td>64,960,000,000</td>\n",
              "      <td>5,960,215</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-04-26</th>\n",
              "      <td>5473</td>\n",
              "      <td>900100</td>\n",
              "      <td>뉴프라이드(Reg.S)</td>\n",
              "      <td>10,650</td>\n",
              "      <td>9,560</td>\n",
              "      <td>10,650</td>\n",
              "      <td>9,510</td>\n",
              "      <td>74,550,000,000</td>\n",
              "      <td>2,521,236</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-04-27</th>\n",
              "      <td>5474</td>\n",
              "      <td>900100</td>\n",
              "      <td>뉴프라이드(Reg.S)</td>\n",
              "      <td>9,510</td>\n",
              "      <td>11,300</td>\n",
              "      <td>11,600</td>\n",
              "      <td>9,430</td>\n",
              "      <td>66,570,000,000</td>\n",
              "      <td>3,643,891</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.981481</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            count_day  ISU_CD        ISU_NM TDD_CLSPRC TDD_OPNPRC TDD_HGPRC  \\\n",
              "Date                                                                          \n",
              "2010-04-21       5468  900100  뉴프라이드(Reg.S)     12,750     15,000    17,250   \n",
              "2010-04-22       5469  900100  뉴프라이드(Reg.S)     10,850     10,850    10,850   \n",
              "2010-04-23       5470  900100  뉴프라이드(Reg.S)      9,280      9,470    10,800   \n",
              "2010-04-26       5473  900100  뉴프라이드(Reg.S)     10,650      9,560    10,650   \n",
              "2010-04-27       5474  900100  뉴프라이드(Reg.S)      9,510     11,300    11,600   \n",
              "\n",
              "           TDD_LWPRC          MKTCAP ACC_TRDVOL  EPS  ...  BPS  PBR  DPS  \\\n",
              "Date                                                  ...                  \n",
              "2010-04-21    12,750  89,250,000,000  2,186,517  NaN  ...  NaN  NaN  NaN   \n",
              "2010-04-22    10,850  75,950,000,000     47,396  NaN  ...  NaN  NaN  NaN   \n",
              "2010-04-23     9,250  64,960,000,000  5,960,215  NaN  ...  NaN  NaN  NaN   \n",
              "2010-04-26     9,510  74,550,000,000  2,521,236  NaN  ...  NaN  NaN  NaN   \n",
              "2010-04-27     9,430  66,570,000,000  3,643,891  NaN  ...  NaN  NaN  NaN   \n",
              "\n",
              "           DVD_YLD GDC_sig  RSI_sig  ROC_sig  MAP_sig  STC_sig     TREND  \n",
              "Date                                                                      \n",
              "2010-04-21     NaN       0        0       -1        1        0       NaN  \n",
              "2010-04-22     NaN       0        0       -1        1        0       NaN  \n",
              "2010-04-23     NaN       0        0       -1        1        0       NaN  \n",
              "2010-04-26     NaN       0        0       -1        1        0  1.000000  \n",
              "2010-04-27     NaN       0        0       -1        1        0  0.981481  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a new 'Date' column by combining 'Y', 'M', 'D' columns\n",
        "total_df['Date'] = pd.to_datetime(total_df[['Y', 'M', 'D']].rename(columns={'Y': 'year', 'M': 'month', 'D': 'day'}))\n",
        "\n",
        "# Set 'Date' as the index\n",
        "total_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Create a 'count_day' column that represents the number of days from the first date\n",
        "total_df['count_day'] = (total_df.index - total_df.index.min()).days\n",
        "\n",
        "# Drop the 'Y', 'M', 'Day' columns as they're no longer needed\n",
        "total_df.drop(columns=['Y', 'M', 'D'], inplace=True)\n",
        "\n",
        "# Reorder the columns to make 'count_day' first\n",
        "cols = ['count_day'] + [col for col in total_df.columns if col != 'count_day']\n",
        "total_df = total_df[cols]\n",
        "\n",
        "total_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.drop(['ISU_CD'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9384645 entries, 0 to 9384644\n",
            "Data columns (total 20 columns):\n",
            " #   Column      Dtype  \n",
            "---  ------      -----  \n",
            " 0   count_day   int64  \n",
            " 1   ISU_NM      object \n",
            " 2   TDD_CLSPRC  object \n",
            " 3   TDD_OPNPRC  object \n",
            " 4   TDD_HGPRC   object \n",
            " 5   TDD_LWPRC   object \n",
            " 6   MKTCAP      object \n",
            " 7   ACC_TRDVOL  object \n",
            " 8   EPS         object \n",
            " 9   PER         object \n",
            " 10  BPS         object \n",
            " 11  PBR         object \n",
            " 12  DPS         object \n",
            " 13  DVD_YLD     object \n",
            " 14  GDC_sig     int64  \n",
            " 15  RSI_sig     int64  \n",
            " 16  ROC_sig     int64  \n",
            " 17  MAP_sig     int64  \n",
            " 18  STC_sig     int64  \n",
            " 19  TREND       float64\n",
            "dtypes: float64(1), int64(6), object(13)\n",
            "memory usage: 1.4+ GB\n"
          ]
        }
      ],
      "source": [
        "total_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPS non-NaN values:\n",
            " 23765          -\n",
            "23766          -\n",
            "23767          -\n",
            "23768          -\n",
            "23769          -\n",
            "           ...  \n",
            "9374643    1,033\n",
            "9374644    1,033\n",
            "9374645    1,033\n",
            "9374646    1,033\n",
            "9374647    1,033\n",
            "Name: EPS, Length: 9125271, dtype: object\n",
            "PER non-NaN values:\n",
            " 23765          -\n",
            "23766          -\n",
            "23767          -\n",
            "23768          -\n",
            "23769          -\n",
            "           ...  \n",
            "9374643    14.09\n",
            "9374644    13.79\n",
            "9374645    14.13\n",
            "9374646    14.13\n",
            "9374647    13.89\n",
            "Name: PER, Length: 9125271, dtype: object\n",
            "BPS non-NaN values:\n",
            " 23765           -\n",
            "23766           -\n",
            "23767           -\n",
            "23768           -\n",
            "23769           -\n",
            "            ...  \n",
            "9374643    11,860\n",
            "9374644    11,860\n",
            "9374645    11,860\n",
            "9374646    11,860\n",
            "9374647    11,860\n",
            "Name: BPS, Length: 9125271, dtype: object\n",
            "PBR non-NaN values:\n",
            " 23765         -\n",
            "23766         -\n",
            "23767         -\n",
            "23768         -\n",
            "23769         -\n",
            "           ... \n",
            "9374643    1.23\n",
            "9374644    1.20\n",
            "9374645    1.23\n",
            "9374646    1.23\n",
            "9374647    1.21\n",
            "Name: PBR, Length: 9125271, dtype: object\n",
            "DPS non-NaN values:\n",
            " 23765        -\n",
            "23766        -\n",
            "23767        -\n",
            "23768        -\n",
            "23769        -\n",
            "          ... \n",
            "9374643    180\n",
            "9374644    180\n",
            "9374645    180\n",
            "9374646    180\n",
            "9374647    180\n",
            "Name: DPS, Length: 9125271, dtype: object\n",
            "DVD_YLD non-NaN values:\n",
            " 23765         -\n",
            "23766         -\n",
            "23767         -\n",
            "23768         -\n",
            "23769         -\n",
            "           ... \n",
            "9374643    1.24\n",
            "9374644    1.26\n",
            "9374645    1.23\n",
            "9374646    1.23\n",
            "9374647    1.25\n",
            "Name: DVD_YLD, Length: 9125271, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Display non-NaN values of the columns to be dropped (for verification)\n",
        "print(\"EPS non-NaN values:\\n\", total_df[\"EPS\"].dropna())\n",
        "print(\"PER non-NaN values:\\n\", total_df[\"PER\"].dropna())\n",
        "print(\"BPS non-NaN values:\\n\", total_df[\"BPS\"].dropna())\n",
        "print(\"PBR non-NaN values:\\n\", total_df[\"PBR\"].dropna())\n",
        "print(\"DPS non-NaN values:\\n\", total_df[\"DPS\"].dropna())\n",
        "print(\"DVD_YLD non-NaN values:\\n\", total_df[\"DVD_YLD\"].dropna())\n",
        "\n",
        "# Drop the unusable columns\n",
        "total_df = total_df.drop([\"EPS\", \"PER\", \"BPS\", \"PBR\", \"DPS\", \"DVD_YLD\"], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique TREND values: {0.0, 1.0, -1.0}\n",
            "TREND value counts:\n",
            "  0.0    8933593\n",
            " 1.0     225720\n",
            "-1.0     225332\n",
            "Name: TREND, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming total_df is already defined and filled with NaN values replaced by 0\n",
        "# total_df = ...\n",
        "\n",
        "# 1) Set TREND to 0 for any value that is not -1, 0, or 1\n",
        "total_df.loc[~total_df[\"TREND\"].isin([-1, 0, 1]), \"TREND\"] = 0\n",
        "\n",
        "# 2) Set TREND to -1 for negative values and 1 for positive values\n",
        "total_df.loc[total_df[\"TREND\"] < 0, \"TREND\"] = -1\n",
        "total_df.loc[total_df[\"TREND\"] > 0, \"TREND\"] = 1\n",
        "\n",
        "# 3) Adjust TREND values based on the specified conditions\n",
        "total_df.loc[total_df[\"TREND\"] <= -0.5, \"TREND\"] = -1\n",
        "total_df.loc[total_df[\"TREND\"] >= 0.5, \"TREND\"] = 1\n",
        "total_df.loc[(total_df[\"TREND\"] > -0.5) & (total_df[\"TREND\"] < 0.5), \"TREND\"] = 0\n",
        "\n",
        "# Check the unique values in the TREND column and their counts\n",
        "unique_trends = set(total_df[\"TREND\"])\n",
        "trend_counts = total_df[\"TREND\"].value_counts()\n",
        "\n",
        "# Print the unique values and their counts\n",
        "print(\"Unique TREND values:\", unique_trends)\n",
        "print(\"TREND value counts:\\n\", trend_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df[\"TREND\"] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0          1\n",
              "1          1\n",
              "2          1\n",
              "3          2\n",
              "4          1\n",
              "          ..\n",
              "9384640    1\n",
              "9384641    1\n",
              "9384642    1\n",
              "9384643    1\n",
              "9384644    1\n",
              "Name: TREND, Length: 9384645, dtype: Int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_df[\"TREND\"] = total_df[\"TREND\"].convert_dtypes(int)\n",
        "total_df[\"TREND\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of columns to convert from strings to numeric values\n",
        "columns_to_convert = [\"TDD_CLSPRC\", \"TDD_OPNPRC\", \"TDD_HGPRC\", \"TDD_LWPRC\", \"MKTCAP\", \"ACC_TRDVOL\"]\n",
        "\n",
        "# Convert the columns to numeric values\n",
        "for col in columns_to_convert:\n",
        "    total_df[col] = total_df[col].str.replace(pat=r'[^0-9]', repl=r'' ,regex=True).apply(pd.to_numeric)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "\n",
        "# Assuming total_df is already defined and filled with NaN values replaced by 0\n",
        "# total_df = ...\n",
        "\n",
        "# Define the scalers\n",
        "mm = MinMaxScaler()\n",
        "sc = RobustScaler()\n",
        "\n",
        "# # Apply MinMax scaling to the specified columns\n",
        "# minmax_cols = [\"count_day\", \"TDD_CLSPRC\", \"TDD_OPNPRC\", \"TDD_HGPRC\", \"TDD_LWPRC\"]\n",
        "# for col in minmax_cols:\n",
        "#     total_df[col] = mm.fit_transform(total_df[col].values.reshape(-1, 1))\n",
        "\n",
        "# # Apply Robust scaling to the specified columns\n",
        "# robust_cols = [\"MKTCAP\", \"ACC_TRDVOL\"]\n",
        "# for col in robust_cols:\n",
        "#     total_df[col] = sc.fit_transform(total_df[col].values.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure 'ISU_NM' is of string type\n",
        "total_df[\"ISU_NM\"] = total_df[\"ISU_NM\"].astype(str)\n",
        "\n",
        "# Calculate where 'ISU_NM' column changes value\n",
        "isu_nm_changes = total_df['ISU_NM'].shift() != total_df['ISU_NM']\n",
        "change_indices = [0] + isu_nm_changes[isu_nm_changes].index.tolist() + [len(total_df)]\n",
        "\n",
        "# Compute pairs of (start, end) indices\n",
        "segment_pairs = [(change_indices[i], change_indices[i+1]) for i in range(len(change_indices) - 1)]\n",
        "\n",
        "print(\"Pairs of (start, end) indices:\", segment_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop ISU_NM if it exists\n",
        "if \"ISU_NM\" in total_df.columns:\n",
        "    total_df = total_df.drop(\"ISU_NM\", axis=1)\n",
        "else:\n",
        "    print(\"Column 'ISU_NM' not found in DataFrame\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "# Convert to Dask DataFrame\n",
        "ddf = dd.from_pandas(total_df, npartitions=10)\n",
        "\n",
        "# Apply pd.to_numeric in a distributed manner\n",
        "ddf_numeric = ddf.map_partitions(lambda df: df.apply(pd.to_numeric, errors='coerce')).compute()\n",
        "\n",
        "ddf_numeric.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def convert_nullable_int_columns(df):\n",
        "\n",
        "    int_columns = df.select_dtypes(include=['Int64']).columns\n",
        "    for col in int_columns:\n",
        "        df[col] = df[col].astype('int64')\n",
        "    return df\n",
        "\n",
        "def gpu_standard_scaler(tensor, dim = 1):\n",
        "    return (tensor - tensor.mean(dim = dim))/(tensor.std(dim = dim) + 1e-8)\n",
        "\n",
        "\n",
        "def process_dataframe(df, segments, use_scale=False, include_diff=False):\n",
        "\n",
        "    df_numeric = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    df_numeric = convert_nullable_int_columns(df_numeric)\n",
        "\n",
        "    df_numeric = df_numeric.dropna()\n",
        "\n",
        "    df_tensor = torch.tensor(df_numeric.values, dtype=torch.float64).cuda()\n",
        "\n",
        "    df_list_diff = []\n",
        "\n",
        "    for start, end in segments:\n",
        "        segment = df_tensor[start:end]\n",
        "\n",
        "        if use_scale:\n",
        "            segment = gpu_standard_scaler(segment, dim = 0)\n",
        "            # scaler = StandardScaler()\n",
        "            # segment = torch.tensor(scaler.fit_transform(segment.cpu()), dtype=torch.float64).cuda()\n",
        "\n",
        "        if include_diff:\n",
        "            segment_diff = segment[1:] - segment[:-1]\n",
        "            df_list_diff.append(segment_diff)\n",
        "\n",
        "    if include_diff:\n",
        "        processed_tensor = torch.cat(df_list_diff, dim=0)\n",
        "    else:\n",
        "        processed_tensor = torch.cat([df_tensor[start:end] for start, end in segments], dim=0)\n",
        "\n",
        "    processed_df = pd.DataFrame(processed_tensor.cpu().numpy(), columns=df_numeric.columns)\n",
        "\n",
        "    new_segment_pairs = [(0, len(segment)) for segment in df_list_diff] if include_diff else segments\n",
        "\n",
        "    return processed_df, new_segment_pairs\n",
        "\n",
        "total_df, segment_pairs = process_dataframe(total_df, segment_pairs, use_scale=True, include_diff=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "segment_pairs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Overview and Usage Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Overview and Usage Guide\n",
        "\n",
        "\"\"\"\n",
        "- Data Overview\n",
        "    Preprocessed Data: total_df\n",
        "    Categorical Columns: Y, M, D, ISU_CD, GDC_sig, RSI_sig, ROC_sig, MAP_sig, STC_sig\n",
        "    Numerical Columns: TDD_CLSPRC, TDD_OPNPRC, TDD_HGPRC, TDD_LWPRC, MKTCAP, ACC_TRDVOL\n",
        "    Label: TREND\n",
        "\n",
        "- Considerations:\n",
        "    1) It is recommended to use embedding techniques for categorical data.\n",
        "    2) Labels:\n",
        "        NaN values have been replaced with 0.\n",
        "\n",
        "        2-1) Label Processing:\n",
        "            How to handle -1, 0, 1 depends on the definition.\n",
        "            ● Classification of -1, 0, 1:\n",
        "                Commonly, the label being discrete is an issue.\n",
        "                (1) Set to -1 for values less than 0, and 1 for values greater than 0.\n",
        "                    # Ratio of -1, 0, 1 = 1397:1440:55\n",
        "                    : This results in very frequent trading.\n",
        "\n",
        "                (2) Use only -1, 0, 1.\n",
        "                    # Ratio of -1, 0, 1 = 76:2740:76\n",
        "                    : This might cause the model to miss buying opportunities when it should, making it difficult for the model to make accurate predictions.\n",
        "\n",
        "                (3) Set to -1 for values less than -0.5, and 1 for values greater than 0.5, otherwise 0.\n",
        "                    # Ratio of -1, 0, 1 = 752:1409:731\n",
        "                    : (Current preprocessing state) This provides a somewhat balanced ratio.\n",
        "\n",
        "            ● Regression:\n",
        "                Keep the label as it is.\n",
        "                (1) The model performs regression and decides whether to buy or sell based on the predicted increase or decrease.\n",
        "\n",
        "    3) The utility of GDC, RSI, ROC, MAP, STC indicators for learning is uncertain.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SequentialDataset(Dataset):\n",
        "    def __init__(self, df, segment_pairs, max_window_size):\n",
        "        self.df = df\n",
        "        self.segment_pairs = segment_pairs\n",
        "        self.max_window_size = max_window_size\n",
        "        self.min_window_size = max_window_size // 2\n",
        "        \n",
        "        # Compute the total number of possible subsequences\n",
        "        self.subsequence_lengths = [\n",
        "            max(0, end - start - self.min_window_size + 1)\n",
        "            for start, end in self.segment_pairs\n",
        "        ]\n",
        "        self.cumulative_lengths = [sum(self.subsequence_lengths[:i+1]) for i in range(len(self.subsequence_lengths))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.cumulative_lengths[-1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Find the appropriate segment for the given idx\n",
        "        segment_idx = next(i for i, cum_len in enumerate(self.cumulative_lengths) if cum_len > idx)\n",
        "        \n",
        "        # Adjust idx to be relative to the found segment\n",
        "        if segment_idx > 0:\n",
        "            idx -= self.cumulative_lengths[segment_idx - 1]\n",
        "        \n",
        "        start_idx, end_idx = self.segment_pairs[segment_idx]\n",
        "        actual_start_idx = start_idx + idx\n",
        "        actual_end_idx = next(end for start, end in self.segment_pairs if start <= actual_start_idx < end)\n",
        "        \n",
        "        window_size = random.randint(self.min_window_size, self.max_window_size)\n",
        "        window_size = min(window_size, actual_end_idx - actual_start_idx)\n",
        "        \n",
        "        seq_end_idx = min(actual_start_idx + window_size, actual_end_idx)\n",
        "\n",
        "        seq = self.df.iloc[actual_start_idx:seq_end_idx]\n",
        "\n",
        "        X = seq.drop(['TREND'], axis=1)\n",
        "        y = seq['TREND']\n",
        "\n",
        "        X = torch.tensor(X.values, dtype=torch.float32)\n",
        "\n",
        "        label = torch.tensor(y.values.astype(int), dtype=torch.long)\n",
        "        label = F.one_hot(label, num_classes=3)\n",
        "\n",
        "        return X, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from random import shuffle\n",
        "\n",
        "# Assuming 'df' and 'num_classes' are defined\n",
        "max_window_size = 64\n",
        "shuffle(segment_pairs)  # Shuffle the indices to randomize the data order\n",
        "train_indices, test_indices = segment_pairs[:int(0.8 * len(segment_pairs))], segment_pairs[int(0.8 * len(segment_pairs)):]\n",
        "\n",
        "trainset = SequentialDataset(df=total_df, indices=train_indices, max_window_size=max_window_size)\n",
        "testset = SequentialDataset(df=total_df, indices=test_indices, max_window_size=max_window_size)\n",
        "\n",
        "print('Train indices: ', len(train_indices))\n",
        "print('Test indices: ', len(test_indices))\n",
        "\n",
        "print(trainset[0][0].shape)\n",
        "print(len(trainset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tools.setting.ml_params import MLParameters\n",
        "from tools.setting.data_config import DataConfig\n",
        "from nn.utils.init import set_random_seed\n",
        "set_random_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_config = DataConfig(dataset_name = 'stock_price', task_type='multi_class_classification', obs_shape=[12], label_size=3)\n",
        "\n",
        "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
        "ml_params = MLParameters(core_model = 'gpt', encoder_model = 'none')\n",
        "\n",
        "first_data = trainset[0]\n",
        "X, y = first_data\n",
        "\n",
        "print(f\"Input shape: {X.shape}\")\n",
        "print(f\"Label shape: {y.shape}\")\n",
        "\n",
        "print(f\"Total number of samples in trainset: {len(trainset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trainer_hub import TrainerHub\n",
        "\n",
        "# Set the device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
        "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer_hub.train(trainset, testset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "metadata": {
      "interpreter": {
        "hash": "a7e81af88087f1f4bdc1f0426df14b24fa2673362c5daa7f7f9146748f40b3b1"
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "b287f80b48e4412a59791e63d64f0b079e04f47b5726df5f54fb3b5044d29a99"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
