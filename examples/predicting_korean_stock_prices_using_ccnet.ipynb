{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "path_append = \"../\"\n",
        "sys.path.append(path_append)  # Go up one directory from where you are.\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import glob\n",
        "import tqdm\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "    TRD_DD : Date\n",
        "    ISU_CD : Stock Code\n",
        "    ISU_NM : Stock Name\n",
        "    TDD_CLSPRC : Closing Price\n",
        "    TDD_OPNPRC : Opening Price\n",
        "    TDD_HGPRC : High Price\n",
        "    TDD_LWPRC : Low Price\n",
        "    MKTCAP : Market Capitalization\n",
        "    ACC_TRDVOL : Trading Volume\n",
        "    EPS : Earnings Per Share\n",
        "    PER : Price-Earnings Ratio\n",
        "    BPS : Book Value Per Share\n",
        "    PBR : Price-Book Ratio\n",
        "    DPS : Dividends Per Share\n",
        "    DVD_YLD : Dividend Yield\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_and_merge_csv_files(data_directory, preprocessed_directory, file_limit=None):\n",
        "    data_files = glob.glob(os.path.join(data_directory, \"*.csv\"))\n",
        "    preprocessed_files = glob.glob(os.path.join(preprocessed_directory, \"*.csv\"))\n",
        "    \n",
        "    data_files = data_files[:file_limit]\n",
        "    preprocessed_files = preprocessed_files[:file_limit]\n",
        "    \n",
        "    merged_dfs = []\n",
        "    \n",
        "    for data_file in data_files:\n",
        "        file_name = os.path.basename(data_file)\n",
        "        \n",
        "        preprocessed_file_name = file_name.replace('.csv', '_preprocessed.csv')\n",
        "        preprocessed_file_path = os.path.join(preprocessed_directory, preprocessed_file_name)\n",
        "        \n",
        "        if preprocessed_file_path in preprocessed_files:\n",
        "            df_data = pd.read_csv(data_file)\n",
        "            df_preprocessed = pd.read_csv(preprocessed_file_path)\n",
        "            \n",
        "            merged_df = pd.merge(df_data, df_preprocessed, on='TRD_DD', suffixes=('_data', '_preprocessed'))\n",
        "            merged_dfs.append(merged_df)\n",
        "    \n",
        "    total_df = pd.concat(merged_dfs, ignore_index=True)\n",
        "    \n",
        "    return total_df\n",
        "\n",
        "data_directory = path_append + \"../data/KR_Data/data\"\n",
        "preprocessed_directory = path_append + \"../data/KR_Data/preprocessed\"\n",
        "total_df = load_and_merge_csv_files(data_directory, preprocessed_directory)\n",
        "\n",
        "\n",
        "total_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming total_df is already defined and merged from previous steps\n",
        "\n",
        "# Reverse the DataFrame to sort dates from past to present\n",
        "total_df = total_df[::-1].reset_index(drop=True)\n",
        "\n",
        "# Split the \"TRD_DD\" column into year, month, and day columns\n",
        "total_df[[\"Y\", \"M\", \"D\"]] = total_df[\"TRD_DD\"].str.split(\"/\", expand=True)\n",
        "\n",
        "# Drop the original \"TRD_DD\" column\n",
        "total_df = total_df.drop(\"TRD_DD\", axis=1)\n",
        "\n",
        "# Rearrange columns to have year, month, and day first\n",
        "total_df = total_df[[\"Y\", \"M\", \"D\"] + total_df.columns[:-3].to_list()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new 'Date' column by combining 'Y', 'M', 'D' columns\n",
        "total_df['Date'] = pd.to_datetime(total_df[['Y', 'M', 'D']].rename(columns={'Y': 'year', 'M': 'month', 'D': 'day'}))\n",
        "\n",
        "# Set 'Date' as the index\n",
        "total_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Create a 'count_day' column that represents the number of days from the first date\n",
        "total_df['count_day'] = (total_df.index - total_df.index.min()).days\n",
        "\n",
        "# Drop the 'Y', 'M', 'Day' columns as they're no longer needed\n",
        "total_df.drop(columns=['Y', 'M', 'D'], inplace=True)\n",
        "\n",
        "# Reorder the columns to make 'count_day' first\n",
        "cols = ['count_day'] + [col for col in total_df.columns if col != 'count_day']\n",
        "total_df = total_df[cols]\n",
        "\n",
        "total_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.drop(['ISU_CD'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display non-NaN values of the columns to be dropped (for verification)\n",
        "print(\"EPS non-NaN values:\\n\", total_df[\"EPS\"].dropna())\n",
        "print(\"PER non-NaN values:\\n\", total_df[\"PER\"].dropna())\n",
        "print(\"BPS non-NaN values:\\n\", total_df[\"BPS\"].dropna())\n",
        "print(\"PBR non-NaN values:\\n\", total_df[\"PBR\"].dropna())\n",
        "print(\"DPS non-NaN values:\\n\", total_df[\"DPS\"].dropna())\n",
        "print(\"DVD_YLD non-NaN values:\\n\", total_df[\"DVD_YLD\"].dropna())\n",
        "\n",
        "# Drop the unusable columns\n",
        "total_df = total_df.drop([\"EPS\", \"PER\", \"BPS\", \"PBR\", \"DPS\", \"DVD_YLD\"], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming total_df is already defined and filled with NaN values replaced by 0\n",
        "# total_df = ...\n",
        "\n",
        "# 1) Set TREND to 0 for any value that is not -1, 0, or 1\n",
        "total_df.loc[~total_df[\"TREND\"].isin([-1, 0, 1]), \"TREND\"] = 0\n",
        "\n",
        "# 2) Set TREND to -1 for negative values and 1 for positive values\n",
        "total_df.loc[total_df[\"TREND\"] < 0, \"TREND\"] = -1\n",
        "total_df.loc[total_df[\"TREND\"] > 0, \"TREND\"] = 1\n",
        "\n",
        "# 3) Adjust TREND values based on the specified conditions\n",
        "total_df.loc[total_df[\"TREND\"] <= -0.5, \"TREND\"] = -1\n",
        "total_df.loc[total_df[\"TREND\"] >= 0.5, \"TREND\"] = 1\n",
        "total_df.loc[(total_df[\"TREND\"] > -0.5) & (total_df[\"TREND\"] < 0.5), \"TREND\"] = 0\n",
        "\n",
        "# Check the unique values in the TREND column and their counts\n",
        "unique_trends = set(total_df[\"TREND\"])\n",
        "trend_counts = total_df[\"TREND\"].value_counts()\n",
        "\n",
        "# Print the unique values and their counts\n",
        "print(\"Unique TREND values:\", unique_trends)\n",
        "print(\"TREND value counts:\\n\", trend_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df[\"TREND\"] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df[\"TREND\"] = total_df[\"TREND\"].convert_dtypes(int)\n",
        "total_df[\"TREND\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of columns to convert from strings to numeric values\n",
        "columns_to_convert = [\"TDD_CLSPRC\", \"TDD_OPNPRC\", \"TDD_HGPRC\", \"TDD_LWPRC\", \"MKTCAP\", \"ACC_TRDVOL\"]\n",
        "\n",
        "# Convert the columns to numeric values\n",
        "for col in columns_to_convert:\n",
        "    total_df[col] = total_df[col].str.replace(pat=r'[^0-9]', repl=r'' ,regex=True).apply(pd.to_numeric)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "\n",
        "# Assuming total_df is already defined and filled with NaN values replaced by 0\n",
        "# total_df = ...\n",
        "\n",
        "# Define the scalers\n",
        "mm = MinMaxScaler()\n",
        "sc = RobustScaler()\n",
        "\n",
        "# Apply MinMax scaling to the specified columns\n",
        "minmax_cols = [\"count_day\", \"TDD_CLSPRC\", \"TDD_OPNPRC\", \"TDD_HGPRC\", \"TDD_LWPRC\"]\n",
        "for col in minmax_cols:\n",
        "    total_df[col] = mm.fit_transform(total_df[col].values.reshape(-1, 1))\n",
        "\n",
        "# Apply Robust scaling to the specified columns\n",
        "robust_cols = [\"MKTCAP\", \"ACC_TRDVOL\"]\n",
        "for col in robust_cols:\n",
        "    total_df[col] = sc.fit_transform(total_df[col].values.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure 'ISU_NM' is of string type\n",
        "total_df[\"ISU_NM\"] = total_df[\"ISU_NM\"].astype(str)\n",
        "\n",
        "# Calculate where 'ISU_NM' column changes value\n",
        "isu_nm_changes = total_df['ISU_NM'].shift() != total_df['ISU_NM']\n",
        "change_indices = [0] + isu_nm_changes[isu_nm_changes].index.tolist() + [len(total_df)]\n",
        "\n",
        "# Compute pairs of (start, end) indices\n",
        "segment_pairs = [(change_indices[i], change_indices[i+1]) for i in range(len(change_indices) - 1)]\n",
        "\n",
        "print(\"Pairs of (start, end) indices:\", segment_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop ISU_NM if it exists\n",
        "if \"ISU_NM\" in total_df.columns:\n",
        "    total_df = total_df.drop(\"ISU_NM\", axis=1)\n",
        "else:\n",
        "    print(\"Column 'ISU_NM' not found in DataFrame\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_numeric = total_df.apply(pd.to_numeric, errors='coerce')\n",
        "df_numeric.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def convert_nullable_int_columns(df):\n",
        "\n",
        "    int_columns = df.select_dtypes(include=['Int64']).columns\n",
        "    for col in int_columns:\n",
        "        df[col] = df[col].astype('int64')\n",
        "    return df\n",
        "\n",
        "def gpu_standard_scaler(tensor, dim = 1):\n",
        "    return (tensor - tensor.mean(dim = dim))/(tensor.std(dim = dim) + 1e-8)\n",
        "\n",
        "\n",
        "def process_dataframe(df, segments, use_scale=False, include_diff=False):\n",
        "\n",
        "    df_numeric = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    df_numeric = convert_nullable_int_columns(df_numeric)\n",
        "\n",
        "    df_numeric = df_numeric.dropna()\n",
        "\n",
        "    df_tensor = torch.tensor(df_numeric.values, dtype=torch.float64).cuda()\n",
        "\n",
        "    df_list_diff = []\n",
        "\n",
        "    for start, end in segments:\n",
        "        segment = df_tensor[start:end]\n",
        "\n",
        "        if use_scale:\n",
        "            segment = gpu_standard_scaler(segment, dim = 0)\n",
        "            # scaler = StandardScaler()\n",
        "            # segment = torch.tensor(scaler.fit_transform(segment.cpu()), dtype=torch.float64).cuda()\n",
        "\n",
        "        if include_diff:\n",
        "            segment_diff = segment[1:] - segment[:-1]\n",
        "            df_list_diff.append(segment_diff)\n",
        "\n",
        "    if include_diff:\n",
        "        processed_tensor = torch.cat(df_list_diff, dim=0)\n",
        "    else:\n",
        "        processed_tensor = torch.cat([df_tensor[start:end] for start, end in segments], dim=0)\n",
        "\n",
        "    processed_df = pd.DataFrame(processed_tensor.cpu().numpy(), columns=df_numeric.columns)\n",
        "\n",
        "    new_segment_pairs = [(0, len(segment)) for segment in df_list_diff] if include_diff else segments\n",
        "\n",
        "    return processed_df, new_segment_pairs\n",
        "\n",
        "total_df, segment_pairs = process_dataframe(total_df, segment_pairs, use_scale=True, include_diff=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "segment_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import plotly.graph_objects as go\n",
        "\n",
        "# # Assuming total_df is already defined and filled with NaN values replaced by 0\n",
        "# # total_df = ...\n",
        "\n",
        "# # Creating the figure\n",
        "# fig = go.Figure()\n",
        "\n",
        "# # Adding the line plot for MKTCAP\n",
        "# fig.add_trace(go.Scatter(x=total_df.index, y=total_df[\"MKTCAP\"], mode='lines', name=\"stay\"))\n",
        "\n",
        "# # Adding scatter plots for buy and sell points\n",
        "# fig.add_trace(go.Scatter(x=total_df[total_df[\"TREND\"] < 0].index, y=total_df[total_df[\"TREND\"] < 0][\"MKTCAP\"], mode=\"markers\", name=\"buy\", marker=dict(color='green')))\n",
        "# fig.add_trace(go.Scatter(x=total_df[total_df[\"TREND\"] > 0].index, y=total_df[total_df[\"TREND\"] > 0][\"MKTCAP\"], mode=\"markers\", name=\"sell\", marker=dict(color='red')))\n",
        "\n",
        "# # Updating layout\n",
        "# fig.update_layout(\n",
        "#     title='Stock Label',\n",
        "#     xaxis=dict(title='Time'),\n",
        "#     yaxis=dict(title='Market Capitalization')\n",
        "# )\n",
        "\n",
        "# # Display the plot\n",
        "# fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Assuming total_df is already defined\n",
        "# # total_df = ...\n",
        "\n",
        "# # Set the figure size\n",
        "# plt.figure(figsize=(25, 8))\n",
        "\n",
        "# # Plot the MKTCAP line\n",
        "# sns.lineplot(x=total_df.index, y=total_df[\"MKTCAP\"], label=\"stay\", color=\"gray\")\n",
        "\n",
        "# # Plot the buy points (TREND < 0)\n",
        "# sns.scatterplot(x=total_df[total_df[\"TREND\"] < 0].index, y=total_df[total_df[\"TREND\"] < 0][\"MKTCAP\"], label=\"buy\", color=\"blue\")\n",
        "\n",
        "# # Plot the sell points (TREND > 0)\n",
        "# sns.scatterplot(x=total_df[total_df[\"TREND\"] > 0].index, y=total_df[total_df[\"TREND\"] > 0][\"MKTCAP\"], label=\"sell\", color=\"red\")\n",
        "\n",
        "# # Add titles and labels\n",
        "# plt.title('Stock Market Capitalization with Buy/Sell Signals')\n",
        "# plt.xlabel('Time')\n",
        "# plt.ylabel('Market Capitalization')\n",
        "\n",
        "# # Show the plot\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Overview and Usage Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Overview and Usage Guide\n",
        "\n",
        "\"\"\"\n",
        "- Data Overview\n",
        "    Preprocessed Data: total_df\n",
        "    Categorical Columns: Y, M, D, ISU_CD, GDC_sig, RSI_sig, ROC_sig, MAP_sig, STC_sig\n",
        "    Numerical Columns: TDD_CLSPRC, TDD_OPNPRC, TDD_HGPRC, TDD_LWPRC, MKTCAP, ACC_TRDVOL\n",
        "    Label: TREND\n",
        "\n",
        "- Considerations:\n",
        "    1) It is recommended to use embedding techniques for categorical data.\n",
        "    2) Labels:\n",
        "        NaN values have been replaced with 0.\n",
        "\n",
        "        2-1) Label Processing:\n",
        "            How to handle -1, 0, 1 depends on the definition.\n",
        "            ● Classification of -1, 0, 1:\n",
        "                Commonly, the label being discrete is an issue.\n",
        "                (1) Set to -1 for values less than 0, and 1 for values greater than 0.\n",
        "                    # Ratio of -1, 0, 1 = 1397:1440:55\n",
        "                    : This results in very frequent trading.\n",
        "\n",
        "                (2) Use only -1, 0, 1.\n",
        "                    # Ratio of -1, 0, 1 = 76:2740:76\n",
        "                    : This might cause the model to miss buying opportunities when it should, making it difficult for the model to make accurate predictions.\n",
        "\n",
        "                (3) Set to -1 for values less than -0.5, and 1 for values greater than 0.5, otherwise 0.\n",
        "                    # Ratio of -1, 0, 1 = 752:1409:731\n",
        "                    : (Current preprocessing state) This provides a somewhat balanced ratio.\n",
        "\n",
        "            ● Regression:\n",
        "                Keep the label as it is.\n",
        "                (1) The model performs regression and decides whether to buy or sell based on the predicted increase or decrease.\n",
        "\n",
        "    3) The utility of GDC, RSI, ROC, MAP, STC indicators for learning is uncertain.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SequentialDataset(Dataset):\n",
        "    def __init__(self, df, indices, max_window_size):\n",
        "        self.df = df\n",
        "        self.indices = indices\n",
        "        self.max_window_size = max_window_size\n",
        "        self.min_window_size = max_window_size // 2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        print(self.indices)\n",
        "        start_idx = self.indices[idx]\n",
        "        # window_size = random.randint(self.min_window_size, self.max_window_size)\n",
        "        window_size = self.max_window_size\n",
        "        end_idx = min(start_idx + window_size, len(self.df))\n",
        "\n",
        "        seq = self.df.iloc[start_idx:end_idx]\n",
        "\n",
        "        X = seq.drop(['TREND'], axis=1)\n",
        "        y = seq['TREND']\n",
        "\n",
        "        X = torch.tensor(X.values, dtype=torch.float32)\n",
        "\n",
        "        label = torch.tensor(y.values.astype(int), dtype=torch.long)\n",
        "        label = F.one_hot(label, num_classes=3)\n",
        "\n",
        "        return X, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from random import shuffle\n",
        "\n",
        "# Assume 'df' is your DataFrame and 'event' is the column containing labels\n",
        "\n",
        "def generate_indices(input_df, input_pairs, max_window_size, test_size=0.2):\n",
        "    length = len(input_pairs)\n",
        "    print(length)\n",
        "    train_length = int(length * (1- test_size))\n",
        "    training_indices = []\n",
        "    testing_indices = []\n",
        "    for iter, (start, end) in enumerate(input_pairs):\n",
        "        indices = training_indices if iter < train_length else testing_indices\n",
        "        max_index = end - max_window_size  # Calculate the maximum starting index for this segment\n",
        "        for i in range(start, max_index):\n",
        "            indices.append(i)\n",
        "            # # Check if all labels in the window are the same\n",
        "            # if len(input_df['TREND'][i:i + max_window_size].unique()) == 1:\n",
        "            #     indices.append(i)\n",
        "            # else:\n",
        "            #     print(f\"Skipping index {i} due to multiple labels in window.\")\n",
        "    return training_indices, testing_indices\n",
        "\n",
        "\n",
        "# Assuming 'df' and 'num_classes' are defined\n",
        "max_window_size = 64\n",
        "shuffle(segment_pairs)  # Shuffle the indices to randomize the data order\n",
        "train_indices, test_indices = generate_indices(total_df, segment_pairs, max_window_size)\n",
        "\n",
        "trainset = SequentialDataset(df=total_df, indices=train_indices, max_window_size=max_window_size)\n",
        "testset = SequentialDataset(df=total_df, indices=test_indices, max_window_size=max_window_size)\n",
        "\n",
        "\n",
        "print('Train indices: ', len(train_indices))\n",
        "print('Test indices: ', len(test_indices))\n",
        "\n",
        "print(trainset[0][0].shape)\n",
        "print(len(trainset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tools.setting.ml_params import MLParameters\n",
        "from tools.setting.data_config import DataConfig\n",
        "from nn.utils.init import set_random_seed\n",
        "set_random_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_config = DataConfig(dataset_name = 'stock_price', task_type='multi_class_classification', obs_shape=[12], label_size=3)\n",
        "\n",
        "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
        "ml_params = MLParameters(core_model = 'gpt', encoder_model = 'none')\n",
        "\n",
        "first_data = trainset[0]\n",
        "X, y = first_data\n",
        "\n",
        "print(f\"Input shape: {X.shape}\")\n",
        "print(f\"Label shape: {y.shape}\")\n",
        "\n",
        "print(f\"Total number of samples in trainset: {len(trainset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trainer_hub import TrainerHub\n",
        "\n",
        "# Set the device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
        "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer_hub.train(trainset, testset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "metadata": {
      "interpreter": {
        "hash": "a7e81af88087f1f4bdc1f0426df14b24fa2673362c5daa7f7f9146748f40b3b1"
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "b287f80b48e4412a59791e63d64f0b079e04f47b5726df5f54fb3b5044d29a99"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
