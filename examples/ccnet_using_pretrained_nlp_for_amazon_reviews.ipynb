{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JoengYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>appVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0899edc2-6dd0-4e40-8471-6836dfc52b00</td>\n",
       "      <td>Quintasha Jackson</td>\n",
       "      <td>I love Amazon ‚ù§Ô∏è</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>28.9.2.100</td>\n",
       "      <td>2024-05-14 23:17:13</td>\n",
       "      <td>28.9.2.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dc8496a1-bb8f-40cd-9ac7-5dc2ba1a6703</td>\n",
       "      <td>Tiffany Boisvert</td>\n",
       "      <td>difficult to figure out</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.7.0.100</td>\n",
       "      <td>2024-05-14 23:16:52</td>\n",
       "      <td>28.7.0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3492103d-2761-4385-b764-d7d2351d6996</td>\n",
       "      <td>Kim Hilliker</td>\n",
       "      <td>wonderful and fast, efficient a d great custom...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>28.9.2.100</td>\n",
       "      <td>2024-05-14 22:41:20</td>\n",
       "      <td>28.9.2.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b49415d0-0f8e-48c9-bf29-718be6cc8b67</td>\n",
       "      <td>Joshua Dickenson</td>\n",
       "      <td>\"Your orders\" screen keeps flashing, can't see...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.9.2.100</td>\n",
       "      <td>2024-05-14 22:39:32</td>\n",
       "      <td>28.9.2.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bee4d6f7-dba0-4895-946e-80432f769eb5</td>\n",
       "      <td>Mohammed Abdalla</td>\n",
       "      <td>üíôüíôüíô</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>28.7.0.100</td>\n",
       "      <td>2024-05-14 22:34:00</td>\n",
       "      <td>28.7.0.100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId           userName  \\\n",
       "0  0899edc2-6dd0-4e40-8471-6836dfc52b00  Quintasha Jackson   \n",
       "1  dc8496a1-bb8f-40cd-9ac7-5dc2ba1a6703   Tiffany Boisvert   \n",
       "2  3492103d-2761-4385-b764-d7d2351d6996       Kim Hilliker   \n",
       "3  b49415d0-0f8e-48c9-bf29-718be6cc8b67   Joshua Dickenson   \n",
       "4  bee4d6f7-dba0-4895-946e-80432f769eb5   Mohammed Abdalla   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0                                   I love Amazon ‚ù§Ô∏è      5              0   \n",
       "1                            difficult to figure out      1              0   \n",
       "2  wonderful and fast, efficient a d great custom...      5              0   \n",
       "3  \"Your orders\" screen keeps flashing, can't see...      1              0   \n",
       "4                                                üíôüíôüíô      5              0   \n",
       "\n",
       "  reviewCreatedVersion                   at  appVersion  \n",
       "0           28.9.2.100  2024-05-14 23:17:13  28.9.2.100  \n",
       "1           28.7.0.100  2024-05-14 23:16:52  28.7.0.100  \n",
       "2           28.9.2.100  2024-05-14 22:41:20  28.9.2.100  \n",
       "3           28.9.2.100  2024-05-14 22:39:32  28.9.2.100  \n",
       "4           28.7.0.100  2024-05-14 22:34:00  28.7.0.100  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../../data/Amazon reviews/amazon_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class PretrainedModelDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, model, num_classes, device, max_length=512, precompute_batches=64, **kwargs):\n",
    "        self.df = df.copy()\n",
    "        self.df['score'] = self.df['score'] - 1\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.precompute_batches = precompute_batches\n",
    "        self.X_cache = None\n",
    "        self.y_cache = None\n",
    "        self.dataset_length = len(self.df)\n",
    "        # Efficiently select a random subset of indices\n",
    "        self.batch_indices = torch.randperm(len(self.df))\n",
    "        self._precompute_batches(0)\n",
    "        \n",
    "    def _precompute_batches(self, start_idx):\n",
    "        end_idx = min(start_idx + self.precompute_batches, self.dataset_length)\n",
    "        batch_indices = self.batch_indices[start_idx:end_idx].tolist()\n",
    "\n",
    "        # Gather batch data\n",
    "        X_batch = [self.df.iloc[i][\"content\"] for i in batch_indices]\n",
    "        y_batch = [self.df.iloc[i][\"score\"] for i in batch_indices]\n",
    "\n",
    "        # Tokenize the batch\n",
    "        X = self.tokenizer(X_batch, truncation=True, max_length=self.max_length, padding='max_length', return_tensors='pt')\n",
    "        \n",
    "        # Move inputs to the correct device\n",
    "        input_ids = X['input_ids'].to(self.device)\n",
    "        attention_mask = X['attention_mask'].to(self.device)\n",
    "\n",
    "        # Get the last hidden state from the RoBERTa model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Reshape the labels for the entire batch\n",
    "        y_one_hot = torch.nn.functional.one_hot(torch.tensor(y_batch, dtype=torch.long), num_classes=self.num_classes)\n",
    "        y_one_hot = y_one_hot.unsqueeze(1).repeat(1, last_hidden_state.size(1), 1)  # Shape: (batch_size, sequence_length, num_classes)\n",
    "        \n",
    "        # Store precomputed batch in cache\n",
    "        self.X_cache = last_hidden_state\n",
    "        self.y_cache = y_one_hot\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = idx // self.precompute_batches\n",
    "        batch_start_idx = batch_idx * self.precompute_batches\n",
    "        cur_idx = idx % self.precompute_batches\n",
    "        if cur_idx == 0:\n",
    "            self._precompute_batches(batch_start_idx)\n",
    "\n",
    "        if cur_idx >= len(self.X_cache):\n",
    "            cur_idx = idx % len(self.X_cache)\n",
    "\n",
    "        X = self.X_cache[cur_idx]\n",
    "        y = self.y_cache[cur_idx]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccn-team\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-irony and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "TARGET_MODEL = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pretrained_model = AutoModel.from_pretrained(TARGET_MODEL).to(device)\n",
    "pretrained_model.eval()\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "train_df, test_df = train_test_split(df, stratify=df[\"score\"], test_size=0.2)\n",
    "num_classes = 5\n",
    "# Create datasets\n",
    "trainset = PretrainedModelDataset(train_df, tokenizer, pretrained_model, num_classes, device, max_length=128)\n",
    "testset = PretrainedModelDataset(test_df, tokenizer, pretrained_model, num_classes, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.setting.data_config import DataConfig\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from trainer_hub import TrainerHub\n",
    "import torch\n",
    "\n",
    "data_config = DataConfig(dataset_name = 'amazon_reviews', task_type='multi_class_classification', obs_shape=[pretrained_model.config.hidden_size], label_size=num_classes)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters(core_model = 'gpt', encoder_model = 'none')\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e20674ec924cdbac6d3b5f5a33cfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad20fe79aaa2407eb0f14cc40a9c87bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][50/633][Time 24.71]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0742\tGen: 0.3940\tRec: 0.3872\tE: 0.0810\tR: 0.0673\tP: 0.7070\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6438\n",
      "precision: 0.2733\n",
      "recall: 0.3588\n",
      "f1_score: 0.3077\n",
      "\n",
      "[0/100][100/633][Time 22.86]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0280\tGen: 0.3219\tRec: 0.3188\tE: 0.0311\tR: 0.0249\tP: 0.6127\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6562\n",
      "precision: 0.2766\n",
      "recall: 0.3608\n",
      "f1_score: 0.3110\n",
      "\n",
      "[0/100][150/633][Time 24.42]\n",
      "Unified LR across all optimizers: 0.00019861406295796434\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0221\tGen: 0.3011\tRec: 0.2993\tE: 0.0240\tR: 0.0203\tP: 0.5783\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6672\n",
      "precision: 0.2973\n",
      "recall: 0.3578\n",
      "f1_score: 0.3180\n",
      "\n",
      "[0/100][200/633][Time 22.84]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0199\tGen: 0.2891\tRec: 0.2877\tE: 0.0213\tR: 0.0184\tP: 0.5569\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6547\n",
      "precision: 0.3018\n",
      "recall: 0.3512\n",
      "f1_score: 0.3143\n",
      "\n",
      "[0/100][250/633][Time 23.77]\n",
      "Unified LR across all optimizers: 0.00019770151423055492\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0164\tGen: 0.2815\tRec: 0.2803\tE: 0.0176\tR: 0.0153\tP: 0.5454\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6766\n",
      "precision: 0.2731\n",
      "recall: 0.3710\n",
      "f1_score: 0.3145\n",
      "\n",
      "[0/100][300/633][Time 23.04]\n",
      "Unified LR across all optimizers: 0.00019724681336564005\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0176\tGen: 0.2759\tRec: 0.2747\tE: 0.0188\tR: 0.0165\tP: 0.5330\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6375\n",
      "precision: 0.2953\n",
      "recall: 0.3483\n",
      "f1_score: 0.3095\n",
      "\n",
      "[0/100][350/633][Time 23.40]\n",
      "Unified LR across all optimizers: 0.00019679315828369438\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0178\tGen: 0.2732\tRec: 0.2719\tE: 0.0191\tR: 0.0164\tP: 0.5273\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6719\n",
      "precision: 0.2841\n",
      "recall: 0.3684\n",
      "f1_score: 0.3187\n",
      "\n",
      "[0/100][400/633][Time 22.58]\n",
      "Unified LR across all optimizers: 0.00019634054657948372\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0159\tGen: 0.2689\tRec: 0.2679\tE: 0.0170\tR: 0.0149\tP: 0.5209\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6812\n",
      "precision: 0.2832\n",
      "recall: 0.3699\n",
      "f1_score: 0.3196\n",
      "\n",
      "[0/100][450/633][Time 23.55]\n",
      "Unified LR across all optimizers: 0.00019588897585330582\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0156\tGen: 0.2635\tRec: 0.2623\tE: 0.0167\tR: 0.0144\tP: 0.5102\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7078\n",
      "precision: 0.3060\n",
      "recall: 0.3710\n",
      "f1_score: 0.3319\n",
      "\n",
      "[0/100][500/633][Time 24.79]\n",
      "Unified LR across all optimizers: 0.00019543844371097777\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0159\tGen: 0.2592\tRec: 0.2580\tE: 0.0171\tR: 0.0148\tP: 0.5012\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6531\n",
      "precision: 0.2786\n",
      "recall: 0.3590\n",
      "f1_score: 0.3106\n",
      "\n",
      "[0/100][550/633][Time 23.00]\n",
      "Unified LR across all optimizers: 0.00019498894776382288\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0159\tGen: 0.2577\tRec: 0.2566\tE: 0.0171\tR: 0.0147\tP: 0.4984\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6719\n",
      "precision: 0.2877\n",
      "recall: 0.3713\n",
      "f1_score: 0.3214\n",
      "\n",
      "[0/100][600/633][Time 24.74]\n",
      "Unified LR across all optimizers: 0.00019454048562865856\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0155\tGen: 0.2533\tRec: 0.2520\tE: 0.0168\tR: 0.0142\tP: 0.4898\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6516\n",
      "precision: 0.2827\n",
      "recall: 0.3622\n",
      "f1_score: 0.3136\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7174828b3b4845d3bafea80cb996c029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100][17/633][Time 25.03]\n",
      "Unified LR across all optimizers: 0.00019409305492778308\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0159\tGen: 0.2517\tRec: 0.2505\tE: 0.0171\tR: 0.0147\tP: 0.4864\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6953\n",
      "precision: 0.3069\n",
      "recall: 0.3588\n",
      "f1_score: 0.3251\n",
      "\n",
      "[1/100][67/633][Time 23.82]\n",
      "Unified LR across all optimizers: 0.00019364665328896346\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0162\tGen: 0.2485\tRec: 0.2472\tE: 0.0175\tR: 0.0149\tP: 0.4795\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6875\n",
      "precision: 0.2975\n",
      "recall: 0.3753\n",
      "f1_score: 0.3285\n",
      "\n",
      "[1/100][117/633][Time 24.28]\n",
      "Unified LR across all optimizers: 0.00019320127834542263\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0172\tGen: 0.2468\tRec: 0.2454\tE: 0.0187\tR: 0.0158\tP: 0.4750\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6766\n",
      "precision: 0.2869\n",
      "recall: 0.3722\n",
      "f1_score: 0.3212\n",
      "\n",
      "[1/100][167/633][Time 23.18]\n",
      "Unified LR across all optimizers: 0.00019275692773582703\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0168\tGen: 0.2446\tRec: 0.2432\tE: 0.0182\tR: 0.0153\tP: 0.4710\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6828\n",
      "precision: 0.2996\n",
      "recall: 0.3731\n",
      "f1_score: 0.3275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(trainset, testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
