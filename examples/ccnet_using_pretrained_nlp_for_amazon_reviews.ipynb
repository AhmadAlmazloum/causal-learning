{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JoengYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>appVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0899edc2-6dd0-4e40-8471-6836dfc52b00</td>\n",
       "      <td>Quintasha Jackson</td>\n",
       "      <td>I love Amazon ‚ù§Ô∏è</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>28.9.2.100</td>\n",
       "      <td>2024-05-14 23:17:13</td>\n",
       "      <td>28.9.2.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dc8496a1-bb8f-40cd-9ac7-5dc2ba1a6703</td>\n",
       "      <td>Tiffany Boisvert</td>\n",
       "      <td>difficult to figure out</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.7.0.100</td>\n",
       "      <td>2024-05-14 23:16:52</td>\n",
       "      <td>28.7.0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3492103d-2761-4385-b764-d7d2351d6996</td>\n",
       "      <td>Kim Hilliker</td>\n",
       "      <td>wonderful and fast, efficient a d great custom...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>28.9.2.100</td>\n",
       "      <td>2024-05-14 22:41:20</td>\n",
       "      <td>28.9.2.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b49415d0-0f8e-48c9-bf29-718be6cc8b67</td>\n",
       "      <td>Joshua Dickenson</td>\n",
       "      <td>\"Your orders\" screen keeps flashing, can't see...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.9.2.100</td>\n",
       "      <td>2024-05-14 22:39:32</td>\n",
       "      <td>28.9.2.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bee4d6f7-dba0-4895-946e-80432f769eb5</td>\n",
       "      <td>Mohammed Abdalla</td>\n",
       "      <td>üíôüíôüíô</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>28.7.0.100</td>\n",
       "      <td>2024-05-14 22:34:00</td>\n",
       "      <td>28.7.0.100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId           userName  \\\n",
       "0  0899edc2-6dd0-4e40-8471-6836dfc52b00  Quintasha Jackson   \n",
       "1  dc8496a1-bb8f-40cd-9ac7-5dc2ba1a6703   Tiffany Boisvert   \n",
       "2  3492103d-2761-4385-b764-d7d2351d6996       Kim Hilliker   \n",
       "3  b49415d0-0f8e-48c9-bf29-718be6cc8b67   Joshua Dickenson   \n",
       "4  bee4d6f7-dba0-4895-946e-80432f769eb5   Mohammed Abdalla   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0                                   I love Amazon ‚ù§Ô∏è      5              0   \n",
       "1                            difficult to figure out      1              0   \n",
       "2  wonderful and fast, efficient a d great custom...      5              0   \n",
       "3  \"Your orders\" screen keeps flashing, can't see...      1              0   \n",
       "4                                                üíôüíôüíô      5              0   \n",
       "\n",
       "  reviewCreatedVersion                   at  appVersion  \n",
       "0           28.9.2.100  2024-05-14 23:17:13  28.9.2.100  \n",
       "1           28.7.0.100  2024-05-14 23:16:52  28.7.0.100  \n",
       "2           28.9.2.100  2024-05-14 22:41:20  28.9.2.100  \n",
       "3           28.9.2.100  2024-05-14 22:39:32  28.9.2.100  \n",
       "4           28.7.0.100  2024-05-14 22:34:00  28.7.0.100  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/datasets/ashishkumarak/amazon-shopping-reviews-daily-updated\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../../data/Amazon reviews/amazon_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class PretrainedModelDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, model, num_classes, device, max_length=512, precompute_batches=64, **kwargs):\n",
    "        self.df = df.copy()\n",
    "        self.df['score'] = self.df['score'] - 1\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.precompute_batches = precompute_batches\n",
    "        self.X_cache = None\n",
    "        self.y_cache = None\n",
    "        self.total_iters = 0\n",
    "        self.dataset_length = len(self.df)\n",
    "        # Efficiently select a random subset of indices\n",
    "        self.batch_indices = torch.randperm(len(self.df))\n",
    "        self._precompute_batches(0)\n",
    "        \n",
    "    def _precompute_batches(self, start_idx):\n",
    "        end_idx = min(start_idx + self.precompute_batches, self.dataset_length)\n",
    "        batch_indices = self.batch_indices[start_idx:end_idx].tolist()\n",
    "\n",
    "        # Gather batch data\n",
    "        X_batch = [self.df.iloc[i][\"content\"] for i in batch_indices]\n",
    "        y_batch = [self.df.iloc[i][\"score\"] for i in batch_indices]\n",
    "\n",
    "        # Tokenize the batch\n",
    "        X = self.tokenizer(X_batch, truncation=True, max_length=self.max_length, padding='max_length', return_tensors='pt')\n",
    "        \n",
    "        # Move inputs to the correct device\n",
    "        input_ids = X['input_ids'].to(self.device)\n",
    "        attention_mask = X['attention_mask'].to(self.device)\n",
    "\n",
    "        # Get the last hidden state from the RoBERTa model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Reshape the labels for the entire batch\n",
    "        y_one_hot = torch.nn.functional.one_hot(torch.tensor(y_batch, dtype=torch.long), num_classes=self.num_classes)\n",
    "        y_one_hot = y_one_hot.unsqueeze(1).repeat(1, last_hidden_state.size(1), 1)  # Shape: (batch_size, sequence_length, num_classes)\n",
    "        \n",
    "        # Store precomputed batch in cache\n",
    "        self.X_cache = last_hidden_state\n",
    "        self.y_cache = y_one_hot\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = idx // self.precompute_batches\n",
    "        batch_start_idx = batch_idx * self.precompute_batches\n",
    "        cur_idx = idx % self.precompute_batches\n",
    "        if self.total_iters % self.precompute_batches == 0:\n",
    "            self._precompute_batches(batch_start_idx)\n",
    "        self.total_iters += 1\n",
    "\n",
    "        if cur_idx >= len(self.X_cache):\n",
    "            cur_idx = idx % len(self.X_cache)\n",
    "\n",
    "        X = self.X_cache[cur_idx]\n",
    "        y = self.y_cache[cur_idx]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-irony and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "TARGET_MODEL = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pretrained_model = AutoModel.from_pretrained(TARGET_MODEL).to(device)\n",
    "pretrained_model.eval()\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "train_df, test_df = train_test_split(df, stratify=df[\"score\"], test_size=0.2)\n",
    "num_classes = 5\n",
    "# Create datasets\n",
    "trainset = PretrainedModelDataset(train_df, tokenizer, pretrained_model, num_classes, device, max_length=128)\n",
    "testset = PretrainedModelDataset(test_df, tokenizer, pretrained_model, num_classes, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from tools.setting.data_config import DataConfig\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from trainer_hub import TrainerHub\n",
    "import torch\n",
    "\n",
    "data_config = DataConfig(dataset_name = 'amazon_reviews', task_type='multi_class_classification', obs_shape=[pretrained_model.config.hidden_size], label_size=num_classes)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters(core_model = 'gpt', encoder_model = 'none')\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101325d8e1454d28a8271c6ed88c25df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431ddb7baba2430092f772e2c24cfcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][50/633][Time 25.45]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0854\tGen: 0.3927\tRec: 0.3853\tE: 0.0928\tR: 0.0781\tP: 0.6925\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.4672\n",
      "precision: 0.0934\n",
      "recall: 0.2000\n",
      "f1_score: 0.1274\n",
      "\n",
      "[0/100][100/633][Time 24.75]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0455\tGen: 0.3203\tRec: 0.3175\tE: 0.0483\tR: 0.0427\tP: 0.5924\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.4781\n",
      "precision: 0.0956\n",
      "recall: 0.2000\n",
      "f1_score: 0.1294\n",
      "\n",
      "[0/100][150/633][Time 24.82]\n",
      "Unified LR across all optimizers: 0.00019861406295796434\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0342\tGen: 0.2975\tRec: 0.2953\tE: 0.0364\tR: 0.0320\tP: 0.5586\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.4797\n",
      "precision: 0.0959\n",
      "recall: 0.2000\n",
      "f1_score: 0.1297\n",
      "\n",
      "[0/100][200/633][Time 24.58]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0273\tGen: 0.2884\tRec: 0.2867\tE: 0.0289\tR: 0.0256\tP: 0.5479\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5891\n",
      "precision: 0.2331\n",
      "recall: 0.3382\n",
      "f1_score: 0.2739\n",
      "\n",
      "[0/100][250/633][Time 24.87]\n",
      "Unified LR across all optimizers: 0.00019770151423055492\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0191\tGen: 0.2803\tRec: 0.2790\tE: 0.0205\tR: 0.0178\tP: 0.5402\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6703\n",
      "precision: 0.2709\n",
      "recall: 0.3684\n",
      "f1_score: 0.3121\n",
      "\n",
      "[0/100][300/633][Time 24.90]\n",
      "Unified LR across all optimizers: 0.00019724681336564005\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0194\tGen: 0.2753\tRec: 0.2740\tE: 0.0207\tR: 0.0181\tP: 0.5299\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6609\n",
      "precision: 0.2776\n",
      "recall: 0.3766\n",
      "f1_score: 0.3183\n",
      "\n",
      "[0/100][350/633][Time 24.94]\n",
      "Unified LR across all optimizers: 0.00019679315828369438\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0188\tGen: 0.2702\tRec: 0.2689\tE: 0.0201\tR: 0.0175\tP: 0.5203\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6578\n",
      "precision: 0.2578\n",
      "recall: 0.3673\n",
      "f1_score: 0.3029\n",
      "\n",
      "[0/100][400/633][Time 24.89]\n",
      "Unified LR across all optimizers: 0.00019634054657948372\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0175\tGen: 0.2624\tRec: 0.2611\tE: 0.0187\tR: 0.0163\tP: 0.5060\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6547\n",
      "precision: 0.2564\n",
      "recall: 0.3658\n",
      "f1_score: 0.3005\n",
      "\n",
      "[0/100][450/633][Time 24.87]\n",
      "Unified LR across all optimizers: 0.00019588897585330582\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0176\tGen: 0.2602\tRec: 0.2589\tE: 0.0189\tR: 0.0163\tP: 0.5016\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7000\n",
      "precision: 0.2957\n",
      "recall: 0.3652\n",
      "f1_score: 0.3242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(trainset, testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
